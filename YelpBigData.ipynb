{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9Zq40uAOcpO"
      },
      "source": [
        "**yelp_academic_dataset_review.json**\n",
        "Contains full review text data including the user_id that wrote the review and the business_id the review is written for.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnXibr2imzY4",
        "outputId": "382920ff-7323-481d-acb8-7739db324e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata (current_repodata.json): done\n",
            "Solving environment: done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /home/ahfeng/anaconda3/envs/bigdata\n",
            "\n",
            "  added / updated specs:\n",
            "    - tqdm\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    attrs-21.2.0               |     pyhd3eb1b0_0          46 KB\n",
            "    bleach-4.0.0               |     pyhd3eb1b0_0         113 KB\n",
            "    importlib-metadata-4.8.2   |   py39h06a4308_0          39 KB\n",
            "    importlib_metadata-4.8.2   |       hd3eb1b0_0          12 KB\n",
            "    jinja2-3.0.2               |     pyhd3eb1b0_0         110 KB\n",
            "    jsonschema-3.2.0           |     pyhd3eb1b0_2          47 KB\n",
            "    jupyter_client-7.0.6       |     pyhd3eb1b0_0          90 KB\n",
            "    jupyter_core-4.9.1         |   py39h06a4308_0          75 KB\n",
            "    markupsafe-2.0.1           |   py39h27cfd23_0          22 KB\n",
            "    mistune-0.8.4              |py39h27cfd23_1000          57 KB\n",
            "    nbconvert-6.1.0            |   py39h06a4308_0         485 KB\n",
            "    packaging-21.3             |     pyhd3eb1b0_0          36 KB\n",
            "    pandocfilters-1.4.3        |   py39h06a4308_1          14 KB\n",
            "    prometheus_client-0.12.0   |     pyhd3eb1b0_0          47 KB\n",
            "    pyparsing-3.0.4            |     pyhd3eb1b0_0          81 KB\n",
            "    pyzmq-22.3.0               |   py39h295c915_2         470 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    terminado-0.9.4            |   py39h06a4308_0          25 KB\n",
            "    testpath-0.5.0             |     pyhd3eb1b0_0          81 KB\n",
            "    traitlets-5.1.1            |     pyhd3eb1b0_0          84 KB\n",
            "    webencodings-0.5.1         |   py39h06a4308_1          20 KB\n",
            "    zipp-3.6.0                 |     pyhd3eb1b0_0          17 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:         1.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  importlib_metadata pkgs/main/noarch::importlib_metadata-4.8.2-hd3eb1b0_0\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  backports-1.0-py_2\n",
            "  backports.functools_lru_cache-1.6.4-pyhd8ed1ab_0\n",
            "  importlib_resources-5.4.0-pyhd8ed1ab_0\n",
            "  pandoc-2.16.2-h7f98852_0\n",
            "  python_abi-3.9-2_cp39\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  pexpect            conda-forge::pexpect-4.8.0-pyh9f0ad1d~ --> pkgs/main::pexpect-4.8.0-pyhd3eb1b0_3\n",
            "  ptyprocess         conda-forge::ptyprocess-0.7.0-pyhd3de~ --> pkgs/main::ptyprocess-0.7.0-pyhd3eb1b0_2\n",
            "  pyzmq              conda-forge::pyzmq-19.0.2-py39hb69f2a~ --> pkgs/main::pyzmq-22.3.0-py39h295c915_2\n",
            "  send2trash         conda-forge::send2trash-1.8.0-pyhd8ed~ --> pkgs/main::send2trash-1.8.0-pyhd3eb1b0_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  async_generator    conda-forge::async_generator-1.10-py_0 --> pkgs/main::async_generator-1.10-pyhd3eb1b0_0\n",
            "  attrs              conda-forge::attrs-21.2.0-pyhd8ed1ab_0 --> pkgs/main::attrs-21.2.0-pyhd3eb1b0_0\n",
            "  backcall           conda-forge::backcall-0.2.0-pyh9f0ad1~ --> pkgs/main::backcall-0.2.0-pyhd3eb1b0_0\n",
            "  bleach             conda-forge::bleach-4.1.0-pyhd8ed1ab_0 --> pkgs/main::bleach-4.0.0-pyhd3eb1b0_0\n",
            "  decorator          conda-forge::decorator-5.1.0-pyhd8ed1~ --> pkgs/main::decorator-5.1.0-pyhd3eb1b0_0\n",
            "  defusedxml         conda-forge::defusedxml-0.7.1-pyhd8ed~ --> pkgs/main::defusedxml-0.7.1-pyhd3eb1b0_0\n",
            "  entrypoints        conda-forge/noarch::entrypoints-0.3-p~ --> pkgs/main/linux-64::entrypoints-0.3-py39h06a4308_0\n",
            "  importlib-metadata conda-forge::importlib-metadata-4.8.2~ --> pkgs/main::importlib-metadata-4.8.2-py39h06a4308_0\n",
            "  ipython            conda-forge::ipython-7.30.1-py39hf3d1~ --> pkgs/main::ipython-7.29.0-py39hb070fc8_0\n",
            "  ipython_genutils   conda-forge::ipython_genutils-0.2.0-p~ --> pkgs/main::ipython_genutils-0.2.0-pyhd3eb1b0_1\n",
            "  jedi               conda-forge::jedi-0.18.1-py39hf3d152e~ --> pkgs/main::jedi-0.18.0-py39h06a4308_1\n",
            "  jinja2             conda-forge::jinja2-3.0.3-pyhd8ed1ab_0 --> pkgs/main::jinja2-3.0.2-pyhd3eb1b0_0\n",
            "  jsonschema         conda-forge::jsonschema-4.2.1-pyhd8ed~ --> pkgs/main::jsonschema-3.2.0-pyhd3eb1b0_2\n",
            "  jupyter_client     conda-forge::jupyter_client-7.1.0-pyh~ --> pkgs/main::jupyter_client-7.0.6-pyhd3eb1b0_0\n",
            "  jupyter_core       conda-forge::jupyter_core-4.9.1-py39h~ --> pkgs/main::jupyter_core-4.9.1-py39h06a4308_0\n",
            "  jupyterlab_pygmen~ conda-forge::jupyterlab_pygments-0.1.~ --> pkgs/main::jupyterlab_pygments-0.1.2-py_0\n",
            "  libsodium          conda-forge::libsodium-1.0.18-h36c2ea~ --> pkgs/main::libsodium-1.0.18-h7b6447c_0\n",
            "  markupsafe         conda-forge::markupsafe-2.0.1-py39h38~ --> pkgs/main::markupsafe-2.0.1-py39h27cfd23_0\n",
            "  matplotlib-inline  conda-forge::matplotlib-inline-0.1.3-~ --> pkgs/main::matplotlib-inline-0.1.2-pyhd3eb1b0_2\n",
            "  mistune            conda-forge::mistune-0.8.4-py39h3811e~ --> pkgs/main::mistune-0.8.4-py39h27cfd23_1000\n",
            "  nbclient           conda-forge::nbclient-0.5.9-pyhd8ed1a~ --> pkgs/main::nbclient-0.5.3-pyhd3eb1b0_0\n",
            "  nbconvert          conda-forge::nbconvert-6.3.0-py39hf3d~ --> pkgs/main::nbconvert-6.1.0-py39h06a4308_0\n",
            "  nbformat           conda-forge::nbformat-5.1.3-pyhd8ed1a~ --> pkgs/main::nbformat-5.1.3-pyhd3eb1b0_0\n",
            "  nest-asyncio       conda-forge::nest-asyncio-1.5.4-pyhd8~ --> pkgs/main::nest-asyncio-1.5.1-pyhd3eb1b0_0\n",
            "  packaging          conda-forge::packaging-21.3-pyhd8ed1a~ --> pkgs/main::packaging-21.3-pyhd3eb1b0_0\n",
            "  pandocfilters      conda-forge/noarch::pandocfilters-1.5~ --> pkgs/main/linux-64::pandocfilters-1.4.3-py39h06a4308_1\n",
            "  parso               conda-forge::parso-0.8.3-pyhd8ed1ab_0 --> pkgs/main::parso-0.8.2-pyhd3eb1b0_0\n",
            "  pickleshare        conda-forge::pickleshare-0.7.5-py_1003 --> pkgs/main::pickleshare-0.7.5-pyhd3eb1b0_1003\n",
            "  prometheus_client  conda-forge::prometheus_client-0.12.0~ --> pkgs/main::prometheus_client-0.12.0-pyhd3eb1b0_0\n",
            "  prompt-toolkit     conda-forge::prompt-toolkit-3.0.23-py~ --> pkgs/main::prompt-toolkit-3.0.20-pyhd3eb1b0_0\n",
            "  pygments           conda-forge::pygments-2.10.0-pyhd8ed1~ --> pkgs/main::pygments-2.10.0-pyhd3eb1b0_0\n",
            "  pyparsing          conda-forge::pyparsing-3.0.6-pyhd8ed1~ --> pkgs/main::pyparsing-3.0.4-pyhd3eb1b0_0\n",
            "  python-dateutil    conda-forge::python-dateutil-2.8.2-py~ --> pkgs/main::python-dateutil-2.8.2-pyhd3eb1b0_0\n",
            "  terminado          conda-forge::terminado-0.12.1-py39hf3~ --> pkgs/main::terminado-0.9.4-py39h06a4308_0\n",
            "  testpath           conda-forge::testpath-0.5.0-pyhd8ed1a~ --> pkgs/main::testpath-0.5.0-pyhd3eb1b0_0\n",
            "  tornado            conda-forge::tornado-6.1-py39h3811e60~ --> pkgs/main::tornado-6.1-py39h27cfd23_0\n",
            "  traitlets          conda-forge::traitlets-5.1.1-pyhd8ed1~ --> pkgs/main::traitlets-5.1.1-pyhd3eb1b0_0\n",
            "  wcwidth            conda-forge::wcwidth-0.2.5-pyh9f0ad1d~ --> pkgs/main::wcwidth-0.2.5-pyhd3eb1b0_0\n",
            "  webencodings       conda-forge/noarch::webencodings-0.5.~ --> pkgs/main/linux-64::webencodings-0.5.1-py39h06a4308_1\n",
            "  zeromq               conda-forge::zeromq-4.3.4-h9c3ff4c_0 --> pkgs/main::zeromq-4.3.4-h2531618_0\n",
            "  zipp                 conda-forge::zipp-3.6.0-pyhd8ed1ab_0 --> pkgs/main::zipp-3.6.0-pyhd3eb1b0_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "terminado-0.9.4      | 25 KB     | ##################################### | 100% \n",
            "jsonschema-3.2.0     | 47 KB     | ##################################### | 100% \n",
            "bleach-4.0.0         | 113 KB    | ##################################### | 100% \n",
            "packaging-21.3       | 36 KB     | ##################################### | 100% \n",
            "testpath-0.5.0       | 81 KB     | ##################################### | 100% \n",
            "jinja2-3.0.2         | 110 KB    | ##################################### | 100% \n",
            "mistune-0.8.4        | 57 KB     | ##################################### | 100% \n",
            "pyzmq-22.3.0         | 470 KB    | ##################################### | 100% \n",
            "send2trash-1.8.0     | 19 KB     | ##################################### | 100% \n",
            "traitlets-5.1.1      | 84 KB     | ##################################### | 100% \n",
            "jupyter_core-4.9.1   | 75 KB     | ##################################### | 100% \n",
            "pyparsing-3.0.4      | 81 KB     | ##################################### | 100% \n",
            "nbconvert-6.1.0      | 485 KB    | ##################################### | 100% \n",
            "markupsafe-2.0.1     | 22 KB     | ##################################### | 100% \n",
            "zipp-3.6.0           | 17 KB     | ##################################### | 100% \n",
            "importlib-metadata-4 | 39 KB     | ##################################### | 100% \n",
            "prometheus_client-0. | 47 KB     | ##################################### | 100% \n",
            "importlib_metadata-4 | 12 KB     | ##################################### | 100% \n",
            "jupyter_client-7.0.6 | 90 KB     | ##################################### | 100% \n",
            "attrs-21.2.0         | 46 KB     | ##################################### | 100% \n",
            "webencodings-0.5.1   | 20 KB     | ##################################### | 100% \n",
            "pandocfilters-1.4.3  | 14 KB     | ##################################### | 100% \n",
            "Preparing transaction: done\n",
            "Verifying transaction: done\n",
            "Executing transaction: done\n"
          ]
        }
      ],
      "source": [
        "!conda update -y tqdm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hixGR2vYOknp"
      },
      "source": [
        "```\n",
        "{\n",
        "    // string, 22 character unique review id\n",
        "    \"review_id\": \"zdSx_SD6obEhz9VrW9uAWA\",\n",
        "\n",
        "    // string, 22 character unique user id, maps to the user in user.json\n",
        "    \"user_id\": \"Ha3iJu77CxlrFm-vQRs_8g\",\n",
        "\n",
        "    // string, 22 character business id, maps to business in business.json\n",
        "    \"business_id\": \"tnhfDv5Il8EaGSXZGiuQGg\",\n",
        "\n",
        "    // integer, star rating\n",
        "    \"stars\": 4,\n",
        "\n",
        "    // string, date formatted YYYY-MM-DD\n",
        "    \"date\": \"2016-03-09\",\n",
        "\n",
        "    // string, the review itself\n",
        "    \"text\": \"Great place to hang out after work: the prices are decent, and the ambience is fun. It's a bit loud, but very lively. The staff is friendly, and the food is good. They have a good selection of drinks.\",\n",
        "\n",
        "    // integer, number of useful votes received\n",
        "    \"useful\": 0,\n",
        "\n",
        "    // integer, number of funny votes received\n",
        "    \"funny\": 0,\n",
        "\n",
        "    // integer, number of cool votes received\n",
        "    \"cool\": 0\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUilXGyMRX-P"
      },
      "source": [
        "# Spark Setup \n",
        "\n",
        "The following blocks of code will set up our Spark connection. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3yDqu-Ac8Us"
      },
      "outputs": [],
      "source": [
        "# Spark setup\n",
        "%%capture\n",
        "!apt install libkrb5-dev\n",
        "!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "!pip install findspark\n",
        "!pip install sparkmagic\n",
        "!pip install pyspark\n",
        "!pip install spacy\n",
        "! pip install pyspark --user\n",
        "! pip install seaborn --user\n",
        "! pip install plotly --user\n",
        "! pip install imageio --user\n",
        "! pip install folium --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGFPGIDRc-ZQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install gcc python-dev libkrb5-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7uV49NHdAam",
        "outputId": "b185c0fd-67f4-4d0f-bff1-2ceb5971a766"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21/12/12 14:34:28 WARN Utils: Your hostname, lambda-quad resolves to a loopback address: 127.0.1.1; using 10.103.79.83 instead (on interface wlp69s0)\n",
            "21/12/12 14:34:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/12/12 14:34:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('bigdata').getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d556ffyxdB9S"
      },
      "outputs": [],
      "source": [
        "%load_ext sparkmagic.magics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QakuYLHQmzY-",
        "outputId": "7a667a37-44f6-4279-9441-5f1a613bb2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[K     |████████████████████████████████| 459 kB 7.0 MB/s eta 0:00:01\n",
            "\u001b[?25hInstalling collected packages: pymongo\n",
            "Successfully installed pymongo-4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqqc16lwdEz3",
        "outputId": "26d7d3e1-1ef4-406d-de65-6a5728978772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandasql in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (0.7.3)\n",
            "Requirement already satisfied: pandas in /home/ahfeng/.local/lib/python3.9/site-packages (from pandasql) (1.3.4)\n",
            "Requirement already satisfied: numpy in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandasql) (1.21.4)\n",
            "Requirement already satisfied: sqlalchemy in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandasql) (1.4.27)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas->pandasql) (2021.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas->pandasql) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->pandasql) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from sqlalchemy->pandasql) (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "#misc\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "from IPython.display import Image as I\n",
        "\n",
        "#NLP\n",
        "import spacy\n",
        "\n",
        "#graph section\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# JSON parsing\n",
        "import json\n",
        "\n",
        "# Pandas SQL\n",
        "!pip install pandasql\n",
        "import pandasql as ps\n",
        "\n",
        "# HTML parsing\n",
        "\n",
        "import urllib\n",
        "\n",
        "# SQLite RDBMS\n",
        "import sqlite3\n",
        "\n",
        "# SparkFiles\n",
        "from pyspark import SparkFiles\n",
        "\n",
        "# NoSQL DB\n",
        "from pymongo import MongoClient\n",
        "from pymongo.errors import DuplicateKeyError, OperationFailure\n",
        "\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "import pyspark\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "try:\n",
        "    if(spark == None):\n",
        "        spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "        sqlContext=SQLContext(spark)\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName('Initial').getOrCreate()\n",
        "    sqlContext=SQLContext(spark)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L_z8zaHlEjA"
      },
      "source": [
        "# Downloading Yelp Data\n",
        "We stored the Yelp review dataset in Google Drive. The following 2 cells will download and unzip the data for use in Colab. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rpJof9fPchx",
        "outputId": "a3185120-0ece-435e-8709-c5f4e09ea353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 1wKAbVYj_OUImwzcYPYrdenVekBpkaZzY into /content/yelp_dataset.tgz... "
          ]
        }
      ],
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1wKAbVYj_OUImwzcYPYrdenVekBpkaZzY',\n",
        "                                    dest_path='/content/yelp_dataset.tgz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHMkTP56Plvy"
      },
      "outputs": [],
      "source": [
        "!tar -xf yelp_dataset.tgz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvIvx9cJRuf4"
      },
      "source": [
        "#Preprocessing and Loading the Yelp Review Dataset into Spark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE8ITzwqdf9k",
        "outputId": "43312ee5-99c2-483e-90af-e90793e8b9e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "21/12/12 14:34:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Create spark dataframes\n",
        "yelp_review_sdf = spark.read.json('yelp_academic_dataset_review.json')\n",
        "business_sdf = spark.read.json('yelp_academic_dataset_business.json')\n",
        "user_sdf = spark.read.json('yelp_academic_dataset_user.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kY53rhG9mZif",
        "outputId": "e2044b79-f12f-4af7-c3c4-15ed18d00f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('business_id', 'string'), ('cool', 'bigint'), ('date', 'string'), ('funny', 'bigint'), ('review_id', 'string'), ('stars', 'double'), ('text', 'string'), ('useful', 'bigint'), ('user_id', 'string')]\n",
            "[('address', 'string'), ('attributes', 'struct<AcceptsInsurance:string,AgesAllowed:string,Alcohol:string,Ambience:string,BYOB:string,BYOBCorkage:string,BestNights:string,BikeParking:string,BusinessAcceptsBitcoin:string,BusinessAcceptsCreditCards:string,BusinessParking:string,ByAppointmentOnly:string,Caters:string,CoatCheck:string,Corkage:string,DietaryRestrictions:string,DogsAllowed:string,DriveThru:string,GoodForDancing:string,GoodForKids:string,GoodForMeal:string,HairSpecializesIn:string,HappyHour:string,HasTV:string,Music:string,NoiseLevel:string,Open24Hours:string,OutdoorSeating:string,RestaurantsAttire:string,RestaurantsCounterService:string,RestaurantsDelivery:string,RestaurantsGoodForGroups:string,RestaurantsPriceRange2:string,RestaurantsReservations:string,RestaurantsTableService:string,RestaurantsTakeOut:string,Smoking:string,WheelchairAccessible:string,WiFi:string>'), ('business_id', 'string'), ('categories', 'string'), ('city', 'string'), ('hours', 'struct<Friday:string,Monday:string,Saturday:string,Sunday:string,Thursday:string,Tuesday:string,Wednesday:string>'), ('is_open', 'bigint'), ('latitude', 'double'), ('longitude', 'double'), ('name', 'string'), ('postal_code', 'string'), ('review_count', 'bigint'), ('stars', 'double'), ('state', 'string')]\n",
            "[('average_stars', 'double'), ('compliment_cool', 'bigint'), ('compliment_cute', 'bigint'), ('compliment_funny', 'bigint'), ('compliment_hot', 'bigint'), ('compliment_list', 'bigint'), ('compliment_more', 'bigint'), ('compliment_note', 'bigint'), ('compliment_photos', 'bigint'), ('compliment_plain', 'bigint'), ('compliment_profile', 'bigint'), ('compliment_writer', 'bigint'), ('cool', 'bigint'), ('elite', 'string'), ('fans', 'bigint'), ('friends', 'string'), ('funny', 'bigint'), ('name', 'string'), ('review_count', 'bigint'), ('useful', 'bigint'), ('user_id', 'string'), ('yelping_since', 'string')]\n"
          ]
        }
      ],
      "source": [
        "#Check types of data in the spark dataframe\n",
        "print(yelp_review_sdf.dtypes)\n",
        "print(business_sdf.dtypes)\n",
        "print(user_sdf.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEMZcRmcn3AF",
        "outputId": "c02db152-c23a-4733-d189-2c8b818a4b01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Check sizes spark dataframes\n",
        "review_size = yelp_review_sdf.count()\n",
        "business_size = business_sdf.count()\n",
        "user_size = user_sdf.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF39xG1GmR95",
        "outputId": "b33bbec5-44d3-4e5f-cf34-532c0943f0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8635403\n",
            "160585\n",
            "2189457\n"
          ]
        }
      ],
      "source": [
        "#Print sizes \n",
        "print(review_size)\n",
        "print(business_size)\n",
        "print(user_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7idRupW8nuWY"
      },
      "outputs": [],
      "source": [
        "#Create views of the dataframes\n",
        "\n",
        "#business_id and user_id will be used to join with other dataframes\n",
        "#text and stars will be used for sentiment analysis\n",
        "#data will be used in EDA\n",
        "yelp_review_sdf.createOrReplaceTempView('yelp')\n",
        "query = '''SELECT text, stars, business_id, date, user_id\n",
        "FROM yelp\n",
        "'''\n",
        "#Filter yelp_review_sdf\n",
        "filtered_yelp_review_sdf = spark.sql(query)\n",
        "\n",
        "business_sdf.createOrReplaceTempView('business')\n",
        "query = '''SELECT business_id, name, state, review_count, stars as business_stars, categories\n",
        "FROM business\n",
        "'''\n",
        "#Filter business_sdf\n",
        "filtered_business_sdf = spark.sql(query)\n",
        "\n",
        "user_sdf.createOrReplaceTempView('user')\n",
        "query = '''SELECT user_id, review_count, average_stars\n",
        "FROM user\n",
        "'''\n",
        "#Filter business_sdf\n",
        "filtered_user_sdf = spark.sql(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ihBRqICmVyR"
      },
      "outputs": [],
      "source": [
        "#Drop NA's from dataframes \n",
        "filtered_yelp_review_sdf = filtered_yelp_review_sdf.dropna()\n",
        "filtered_business_sdf = filtered_business_sdf.dropna()\n",
        "filtered_user_sdf = filtered_user_sdf.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Ie8TgImeRp"
      },
      "source": [
        "#Visualization 1 (Review Dataset)\n",
        "Let's check the distribution of stars in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpIJwV-pmf2R",
        "outputId": "3b05680c-3937-4596-8a5f-eaad790f08f2"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'cleaned_yelp_review_sdf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_12866/512767555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Group by stars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstar_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_yelp_review_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstar_distribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cleaned_yelp_review_sdf' is not defined"
          ]
        }
      ],
      "source": [
        "#Group by stars\n",
        "star_distribution = cleaned_yelp_review_sdf.groupBy('stars').count()\n",
        "star_distribution.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZiOotbemhQE"
      },
      "outputs": [],
      "source": [
        "#Convert to pandas to create barplot\n",
        "star_distribution_df = star_distribution.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0vHzmkBmqS7"
      },
      "outputs": [],
      "source": [
        "#Scale counts to millions\n",
        "star_distribution_df['count']  = star_distribution_df['count'] / 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scvnscs8msTG"
      },
      "outputs": [],
      "source": [
        "#Check mean value of stars \n",
        "from pyspark.sql.functions import mean as _mean, col\n",
        "\n",
        "stars_mean = cleaned_yelp_review_sdf.select(\n",
        "    _mean(col('stars')).alias('mean')).collect()\n",
        "\n",
        "mean = stars_mean[0]['mean']\n",
        "print(\"Mean stars for a review: \", mean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLKj_DNam5Dq"
      },
      "source": [
        "##Distribution of Stars in Yelp Review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BwPRmRGm5kK"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "dist = sns.barplot(x = star_distribution_df['stars'], y = star_distribution_df['count'], color=\"salmon\", saturation=.5)\n",
        "dist.set(xlabel='Star Ratings', ylabel='Count (in millions)', title=\"Distribution of Stars in Yelp Reviews\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkmFM83om95E"
      },
      "source": [
        "Above we see users are almost 2x as likely to leave a 5 star review than the next most popular rating (4 stars)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMAFMhqcm_4z"
      },
      "source": [
        "#Visualization 2 (Review Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoprBhwInC1Q"
      },
      "source": [
        "Let's check the age of the data to get a sense of when these reviews were written\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJsMgMAsn8il"
      },
      "source": [
        "###Reviews Per Month\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoO8QKr2nAan"
      },
      "outputs": [],
      "source": [
        "filtered_yelp_review_sdf.createOrReplaceTempView(\"table\")\n",
        "query = \"\"\"\n",
        "SELECT stars, TO_DATE(date) as date\n",
        "FROM table\n",
        "\"\"\"\n",
        "\n",
        "review_date_sdf = spark.sql(query)\n",
        "\n",
        "review_date_sdf.createOrReplaceTempView(\"dates\")\n",
        "query = \"\"\"\n",
        "SELECT MONTH(date) as month, YEAR(date) as year, COUNT(stars) as num_reviews\n",
        "FROM dates\n",
        "GROUP BY MONTH(date), YEAR(date)\n",
        "\"\"\"\n",
        "\n",
        "dates_sdf = spark.sql(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9BD1SpjnIZH"
      },
      "outputs": [],
      "source": [
        "dates_sdf.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egHKyb5unKGl"
      },
      "outputs": [],
      "source": [
        "#Convert to pandas\n",
        "dates_df = dates_sdf.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNVS5PFdnN6G"
      },
      "outputs": [],
      "source": [
        "#Convert to datetime\n",
        "dates_df['date'] = pd.to_datetime(dates_df[['year', 'month']].assign(DAY=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ryjeizJnSyi"
      },
      "source": [
        "Let's see the date of the first and last review in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDIYHS0-nROB"
      },
      "outputs": [],
      "source": [
        "first_review_date = min(dates_df['date'])\n",
        "last_review_date = max(dates_df['date'])\n",
        "print(first_review_date)\n",
        "print(last_review_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_j24_J-ngHO"
      },
      "outputs": [],
      "source": [
        "# PLOTTING reviews per month \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (20,12))    \n",
        "fig = sns.barplot(x = 'date', y = \"num_reviews\", data = dates_df, \n",
        "                  estimator = sum, ci = None, ax=ax)\n",
        "\n",
        "x_dates = dates_df['date'].dt.strftime('%Y-%m-%d').sort_values().unique()\n",
        "ax.set_xticklabels(labels=x_dates, rotation=45, ha='right')\n",
        "\n",
        "new_ticks = [i.get_text() for i in fig.get_xticklabels()]\n",
        "plt.xticks(range(0, len(new_ticks), 6), new_ticks[::6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ky2QAcWVntYB"
      },
      "source": [
        "Hard to see trends in the data when plotting per month. Let's check out the reviews per year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyBBylr7oBAV"
      },
      "source": [
        "###Reviews Per Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjPEmoYJoItf"
      },
      "outputs": [],
      "source": [
        "review_date_sdf.createOrReplaceTempView(\"dates\")\n",
        "query = \"\"\"\n",
        "SELECT YEAR(date) as year, COUNT(stars) as num_reviews\n",
        "FROM dates\n",
        "GROUP BY YEAR(date)\n",
        "\"\"\"\n",
        "\n",
        "date_year_sdf = spark.sql(query)\n",
        "\n",
        "date_year_df = date_year_sdf.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSemEP6roGkF"
      },
      "outputs": [],
      "source": [
        "#Convert to datetime\n",
        "date_year_df['date'] = pd.to_datetime(dates_df[['year']].assign(MONTH=1,DAY=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWOnbAPaoN91"
      },
      "outputs": [],
      "source": [
        "#Generate plot \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (20,12))    \n",
        "fig = sns.barplot(x = 'year', y = \"num_reviews\", data = date_year_df, \n",
        "                   ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv0BFiFooQi4"
      },
      "source": [
        "Here we get a better idea of the timeframe the reviews are coming from. 2020 is an outlier because of the Covid-19 outbreak, and data was only partially recorded for 2021.\n",
        "\n",
        "Add in another graph from the user data. Check yelping_since\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JviQJiTTobV9"
      },
      "source": [
        "#Visualization 3 (User Datset)\n",
        "\n",
        "This ended up not working due to extreme outliers. Most users left less than 100 reviews, while some left as many as 15,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cSKLsX7oh3i"
      },
      "outputs": [],
      "source": [
        "#Check the distribution of users's contributions to the dataset\n",
        "review_count_df = filtered_user_sdf.select('review_count').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRZsmU7AojJW"
      },
      "outputs": [],
      "source": [
        "#Summary of the df\n",
        "review_count_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn9Xd0Ejos8q"
      },
      "source": [
        "#Visualization 4 (Business Dataset)\n",
        "Let's see where our data is coming from "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b2tFQbGo3RE"
      },
      "source": [
        "##Reviews per State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8m5QWX56oz5v"
      },
      "outputs": [],
      "source": [
        "filtered_business_sdf.createOrReplaceTempView('bus')\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT state, COUNT(review_count) as review_count\n",
        "FROM bus\n",
        "GROUP BY state\n",
        "\"\"\"\n",
        "\n",
        "state_sdf = spark.sql(query)\n",
        "state_df = state_sdf.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBpcwogco6ba"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px  # Be sure to import express\n",
        "fig = px.choropleth(state_df,  # Input Pandas DataFrame\n",
        "                    locations=\"state\",  # DataFrame column with locations\n",
        "                    color=\"review_count\",  # DataFrame column with color values\n",
        "                    hover_name=\"state\", # DataFrame column hover info\n",
        "                    locationmode = 'USA-states') # Set to plot as US States\n",
        "fig.update_layout(\n",
        "    title_text = 'Reviews per State', # Create a Title\n",
        "    geo_scope='usa',  # Plot only the USA instead of globe\n",
        ")\n",
        "fig.show()  # Output the plot to the screen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6UFsks7o9Od"
      },
      "source": [
        "##Distribution of Star Counts for Businesses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20p2sMtKpBVf"
      },
      "outputs": [],
      "source": [
        "filtered_business_sdf.createOrReplaceTempView('bus')\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT COUNT(business_id) as count, business_stars\n",
        "FROM bus\n",
        "GROUP BY business_stars\n",
        "\"\"\"\n",
        "\n",
        "business_stars_df = spark.sql(query).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iYUM4OipDIW"
      },
      "outputs": [],
      "source": [
        "sns.barplot(data=business_stars_df, x = 'business_stars', y = 'count')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZp7ThsnpIY6"
      },
      "source": [
        "#Good Business"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zHNAJ6cpLEA"
      },
      "source": [
        "##20 Most Reviewed Businesses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEtW31pcpNHm"
      },
      "outputs": [],
      "source": [
        "filtered_business_sdf.createOrReplaceTempView('bus')\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT name, review_count as num_reviews, business_stars\n",
        "FROM bus\n",
        "ORDER BY review_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "most_reviewed_business_df = spark.sql(query).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLCmCv3dpPlw"
      },
      "outputs": [],
      "source": [
        "most_reviewed_business_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVYEvSGgpUdX"
      },
      "source": [
        "##20 Most Reviewed Businesses with a 5 star rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwb4YPbipVr7"
      },
      "outputs": [],
      "source": [
        "filtered_business_sdf.createOrReplaceTempView('bus')\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT name, review_count as num_reviews, business_stars, categories\n",
        "FROM bus\n",
        "WHERE business_stars = 5\n",
        "ORDER BY review_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "most_popular_business_df = spark.sql(query).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG082SBHpXSW"
      },
      "outputs": [],
      "source": [
        "most_popular_business_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNt-8kaIpc7r"
      },
      "source": [
        "##Popular Categories of the Top Rated Businesses in the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoIkq6nwpkt5"
      },
      "source": [
        "###Cleaning and Processing Categories "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXN2xipLpfSx"
      },
      "outputs": [],
      "source": [
        "#Explode the categories column \n",
        "mpb_df = pd.DataFrame(most_popular_business_df.categories.str.split(',').tolist(), index=most_popular_business_df.name).stack()\n",
        "mpb_df = mpb_df.reset_index()[[0, 'name']] # var1 variable is currently labeled 0\n",
        "mpb_df.columns = ['category', 'name'] # renaming var1\n",
        "mpb_df = mpb_df.astype(str)\n",
        "mpb_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWMBXvvhphMK"
      },
      "outputs": [],
      "source": [
        "#Merge categories and popular businesses\n",
        "most_popular_categories_df = mpb_df.merge(most_popular_business_df, on = 'name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKXX-taIpsW4"
      },
      "outputs": [],
      "source": [
        "#Drop Unnecessary column \n",
        "most_popular_categories_df = most_popular_categories_df.drop(['categories'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T950EQHTptBL"
      },
      "outputs": [],
      "source": [
        "#Strip Whitespace\n",
        "most_popular_categories_df['category'] = most_popular_categories_df['category'].str.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfK_kW2xp2N6"
      },
      "source": [
        "###10 Most Popular Categories Among the 20 Most Reviewed Businesses with 5 Star Ratings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV7miuIRp1fy"
      },
      "outputs": [],
      "source": [
        "#get top 10 most popular categories\n",
        "n = 10\n",
        "most_popular_categories_df['category'].value_counts()[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm5UkrS9p9Uw"
      },
      "source": [
        "#Bad for Businesss\n",
        "Let's see what makes a business bad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3GWanmdqCCv"
      },
      "source": [
        "##20 Most Reviewed Businesses with Less than 1.5 Stars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVafi-RGqEna"
      },
      "outputs": [],
      "source": [
        "filtered_business_sdf.createOrReplaceTempView('bus')\n",
        "\n",
        "query = \"\"\"\n",
        "SELECT name, review_count as num_reviews, business_stars, categories\n",
        "FROM bus\n",
        "WHERE business_stars <= 1.5\n",
        "ORDER BY review_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "least_popular_business_df = spark.sql(query).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K94F1E39qJOh"
      },
      "outputs": [],
      "source": [
        "least_popular_business_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXpjLY-WqLdW"
      },
      "source": [
        "##Popular Categories of the Worst Rated Businesses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa2bwzjTqPlC"
      },
      "source": [
        "###Cleaning and Processing Categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQLVnVsNqTE-"
      },
      "outputs": [],
      "source": [
        "#Explode the categories column \n",
        "lpb_df = pd.DataFrame(least_popular_business_df.categories.str.split(',').tolist(), index=least_popular_business_df.name).stack()\n",
        "lpb_df = lpb_df.reset_index()[[0, 'name']] # var1 variable is currently labeled 0\n",
        "lpb_df.columns = ['category', 'name'] # renaming var1\n",
        "lpb_df = lpb_df.astype(str)\n",
        "lpb_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhapmbR-qKup"
      },
      "outputs": [],
      "source": [
        "#Merge categories and popular businesses\n",
        "least_popular_business_df = lpb_df.merge(least_popular_business_df, on = 'name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke_8zJCMqVCW"
      },
      "outputs": [],
      "source": [
        "#Drop columns \n",
        "least_popular_business_df = least_popular_business_df.drop(['categories'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0jwKD3TqXTX"
      },
      "outputs": [],
      "source": [
        "#Strip whitespace \n",
        "least_popular_business_df['category'] = least_popular_business_df['category'].str.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4cDWWgAqbgp"
      },
      "outputs": [],
      "source": [
        "least_popular_business_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhm9iv7Qqgv7"
      },
      "source": [
        "###10 Most Popular Categories Among the 20 Most Reviewed Businesses with 5 Star Ratings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nckDBM4MqhzP"
      },
      "outputs": [],
      "source": [
        "# get top 10 least popular categories\n",
        "n = 10\n",
        "least_popular_business_df['category'].value_counts()[:n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xQ23HXSqkOi"
      },
      "source": [
        "Looks like these are the categories of larger corporations. Also seems like travel related businesses tend to get the worst ratings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o04ocA3OmzZT"
      },
      "outputs": [],
      "source": [
        "sentiment_query = '''SELECT text, stars\n",
        "FROM yelp\n",
        "WHERE stars<>3\n",
        "'''\n",
        "yelp_sdf = spark.sql(sentiment_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1XQtMNKUP_q",
        "outputId": "589495a2-9902-4171-823b-083d0c97a260"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#Added\n",
        "#Sample here to work in pandas for rest of notebook\n",
        "sampled_sdf = yelp_sdf.sample(fraction=.05,seed=69)\n",
        "yelp_df = sampled_sdf.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk7gilFdUV_z"
      },
      "outputs": [],
      "source": [
        "#Added\n",
        "#Lower case all reviews\n",
        "yelp_df['cleaned_reviews'] = yelp_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO8VZEM9UdWe",
        "outputId": "e65aa0f3-51ad-4f57-f434-93c98009845e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_12866/1236021097.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  yelp_df['cleaned_reviews'] = yelp_df['cleaned_reviews'].str.replace('[^\\w\\s]','')\n"
          ]
        }
      ],
      "source": [
        "#Added\n",
        "#Remove punctuation from all reviews \n",
        "yelp_df['cleaned_reviews'] = yelp_df['cleaned_reviews'].str.replace('[^\\w\\s]','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axCs8Gyq1Y1r"
      },
      "outputs": [],
      "source": [
        "#use spacy to tokenize\n",
        "# !python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# yelp_df['tokens'] = yelp_df['text'].apply(nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoWKlR9q4iT4",
        "outputId": "e016251c-bbe8-4fa0-bfee-72e98c22d832"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was not that great. We sat for like 15 mins be...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>was not that great we sat for like 15 mins bef...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My brother in law suggested this place, and we...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>my brother in law suggested this place and we ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We were seated immediately but unfortunately i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>we were seated immediately but unfortunately i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Here's a breakdown of what my boyfriend and I ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>heres a breakdown of what my boyfriend and i s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eating healthy I had the turkey burger it was ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating healthy i had the turkey burger it was ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385393</th>\n",
              "      <td>King burrito is my go to Burritos when I'm nea...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>king burrito is my go to burritos when im near...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385394</th>\n",
              "      <td>Wow, such a lovely spot right off the highway ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>wow such a lovely spot right off the highway t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385395</th>\n",
              "      <td>Where do I start with Buranko? Everything I tr...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>where do i start with buranko everything i tri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385396</th>\n",
              "      <td>Wouldn't even let a pregnant woman use the bat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>wouldnt even let a pregnant woman use the bath...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385397</th>\n",
              "      <td>Best pizza in the neighborhood!!! Love the thi...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>best pizza in the neighborhood love the this c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>385398 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  stars  \\\n",
              "0       Was not that great. We sat for like 15 mins be...    2.0   \n",
              "1       My brother in law suggested this place, and we...    1.0   \n",
              "2       We were seated immediately but unfortunately i...    1.0   \n",
              "3       Here's a breakdown of what my boyfriend and I ...    2.0   \n",
              "4       Eating healthy I had the turkey burger it was ...    4.0   \n",
              "...                                                   ...    ...   \n",
              "385393  King burrito is my go to Burritos when I'm nea...    5.0   \n",
              "385394  Wow, such a lovely spot right off the highway ...    5.0   \n",
              "385395  Where do I start with Buranko? Everything I tr...    5.0   \n",
              "385396  Wouldn't even let a pregnant woman use the bat...    1.0   \n",
              "385397  Best pizza in the neighborhood!!! Love the thi...    5.0   \n",
              "\n",
              "                                          cleaned_reviews  \n",
              "0       was not that great we sat for like 15 mins bef...  \n",
              "1       my brother in law suggested this place and we ...  \n",
              "2       we were seated immediately but unfortunately i...  \n",
              "3       heres a breakdown of what my boyfriend and i s...  \n",
              "4       eating healthy i had the turkey burger it was ...  \n",
              "...                                                   ...  \n",
              "385393  king burrito is my go to burritos when im near...  \n",
              "385394  wow such a lovely spot right off the highway t...  \n",
              "385395  where do i start with buranko everything i tri...  \n",
              "385396  wouldnt even let a pregnant woman use the bath...  \n",
              "385397  best pizza in the neighborhood love the this c...  \n",
              "\n",
              "[385398 rows x 3 columns]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yelp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBZch3NJn8d6"
      },
      "outputs": [],
      "source": [
        "# text_and_stars_sdf = spark.sql(query)\n",
        "# text_and_stars_sdf.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEpxJ1cfoEZX"
      },
      "outputs": [],
      "source": [
        "#View the length of our dataframe \n",
        "# text_and_stars_sdf.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjtH4m8lf9V8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# text_and_stars_sdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpK5Y8iLSXQP"
      },
      "source": [
        "#Cleaning the Yelp Review Dataset\n",
        "\n",
        "For our analysis we are only interested in the text of the reviews and the stars that the user rates the review.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl5nJ7yifD0q"
      },
      "outputs": [],
      "source": [
        "#Cleaning text in reviews\n",
        "#Step 1: Convert all text in each review to lower case\n",
        "\n",
        "# from pyspark.sql import functions as F\n",
        "\n",
        "# columnName = \"text\";\n",
        "# cleaned_sdf = text_and_stars_sdf.withColumn(\"lowerCaseText\", F.lower(F.col(\"text\")));\n",
        "# cleaned_sdf.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uvDmOTVgzKr"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
        "\n",
        "# #Tokenize the text \n",
        "# tokenizer = Tokenizer(inputCol=\"lowerCaseText\", outputCol=\"tokens\")\n",
        "# tokenized = tokenizer.transform(cleaned_sdf)\n",
        "\n",
        "# #Remove stop words \n",
        "# remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"cleanTokens\")\n",
        "# filtered_sdf = remover.transform(tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkdM4xGiinkb"
      },
      "outputs": [],
      "source": [
        "# filtered_sdf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05uSNov_Sp2h"
      },
      "source": [
        "***Extract text and star count columns.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJHcehBS3Xe"
      },
      "source": [
        "***We are only interested in positive and negative reviews for this project. We classify negative reviews as those that receive 1 or 2 star ratings and we classify positive reviews as those that receive 4 or 5 star reviews. Here we will remove reviews with 3 star reviews.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsQmk-fUJp6"
      },
      "source": [
        "# Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wL3pg3HUVp4"
      },
      "source": [
        "##Use regex to clean up the text body of each review. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ4xQ570VKbg"
      },
      "source": [
        "#Exploratory Data Analysis \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwwbXFGbO3vt"
      },
      "source": [
        "#Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTBz5PvFO65a"
      },
      "source": [
        "##Load the data into pandas\n",
        "We load a sample of the dataset into pandas to use for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-JLa6hqPFgY"
      },
      "outputs": [],
      "source": [
        "# sampled_sdf = filtered_sdf.sample(fraction=.05,seed=69)\n",
        "# review_df = sampled_sdf.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVRwZp4dUNS2"
      },
      "outputs": [],
      "source": [
        "review_df = yelp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV6Gz6klmzZa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7hFjYMKZVkn"
      },
      "outputs": [],
      "source": [
        "review_df['sentiment'] = review_df['stars'].apply(lambda x: np.sign(x-3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCgcoB2oUlKO",
        "outputId": "924da9f9-8b23-4172-d298-323636cd9742"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was not that great. We sat for like 15 mins be...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>was not that great we sat for like 15 mins bef...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My brother in law suggested this place, and we...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>my brother in law suggested this place and we ...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We were seated immediately but unfortunately i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>we were seated immediately but unfortunately i...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Here's a breakdown of what my boyfriend and I ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>heres a breakdown of what my boyfriend and i s...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eating healthy I had the turkey burger it was ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating healthy i had the turkey burger it was ...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385393</th>\n",
              "      <td>King burrito is my go to Burritos when I'm nea...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>king burrito is my go to burritos when im near...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385394</th>\n",
              "      <td>Wow, such a lovely spot right off the highway ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>wow such a lovely spot right off the highway t...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385395</th>\n",
              "      <td>Where do I start with Buranko? Everything I tr...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>where do i start with buranko everything i tri...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385396</th>\n",
              "      <td>Wouldn't even let a pregnant woman use the bat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>wouldnt even let a pregnant woman use the bath...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385397</th>\n",
              "      <td>Best pizza in the neighborhood!!! Love the thi...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>best pizza in the neighborhood love the this c...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>385398 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  stars  \\\n",
              "0       Was not that great. We sat for like 15 mins be...    2.0   \n",
              "1       My brother in law suggested this place, and we...    1.0   \n",
              "2       We were seated immediately but unfortunately i...    1.0   \n",
              "3       Here's a breakdown of what my boyfriend and I ...    2.0   \n",
              "4       Eating healthy I had the turkey burger it was ...    4.0   \n",
              "...                                                   ...    ...   \n",
              "385393  King burrito is my go to Burritos when I'm nea...    5.0   \n",
              "385394  Wow, such a lovely spot right off the highway ...    5.0   \n",
              "385395  Where do I start with Buranko? Everything I tr...    5.0   \n",
              "385396  Wouldn't even let a pregnant woman use the bat...    1.0   \n",
              "385397  Best pizza in the neighborhood!!! Love the thi...    5.0   \n",
              "\n",
              "                                          cleaned_reviews  sentiment  \n",
              "0       was not that great we sat for like 15 mins bef...       -1.0  \n",
              "1       my brother in law suggested this place and we ...       -1.0  \n",
              "2       we were seated immediately but unfortunately i...       -1.0  \n",
              "3       heres a breakdown of what my boyfriend and i s...       -1.0  \n",
              "4       eating healthy i had the turkey burger it was ...        1.0  \n",
              "...                                                   ...        ...  \n",
              "385393  king burrito is my go to burritos when im near...        1.0  \n",
              "385394  wow such a lovely spot right off the highway t...        1.0  \n",
              "385395  where do i start with buranko everything i tri...        1.0  \n",
              "385396  wouldnt even let a pregnant woman use the bath...       -1.0  \n",
              "385397  best pizza in the neighborhood love the this c...        1.0  \n",
              "\n",
              "[385398 rows x 4 columns]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "review_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnW40shzeom5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCfnugtdUHce"
      },
      "source": [
        "##Afinn Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSeLa5F9UJeH",
        "outputId": "af50ed2b-9449-4ac5-d800-4833eef197fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting afinn\n",
            "  Using cached afinn-0.1.tar.gz (52 kB)\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53448 sha256=3bff2cfb4107660c9f304ae4436fd235fb2d04c9d36330cd6cb4fdf4a6948df3\n",
            "  Stored in directory: /home/ahfeng/.cache/pip/wheels/79/91/ee/8374d9bc8c6c0896a2db75afdfd63d43653902407a0e76cd94\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install afinn\n",
        "from afinn import Afinn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nQkzPcvUTxR",
        "outputId": "02f82f53-2108-45d4-9700-b85f78a1fb62"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>afinn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was not that great. We sat for like 15 mins be...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>was not that great we sat for like 15 mins bef...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My brother in law suggested this place, and we...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>my brother in law suggested this place and we ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We were seated immediately but unfortunately i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>we were seated immediately but unfortunately i...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Here's a breakdown of what my boyfriend and I ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>heres a breakdown of what my boyfriend and i s...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eating healthy I had the turkey burger it was ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating healthy i had the turkey burger it was ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385393</th>\n",
              "      <td>King burrito is my go to Burritos when I'm nea...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>king burrito is my go to burritos when im near...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385394</th>\n",
              "      <td>Wow, such a lovely spot right off the highway ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>wow such a lovely spot right off the highway t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385395</th>\n",
              "      <td>Where do I start with Buranko? Everything I tr...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>where do i start with buranko everything i tri...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385396</th>\n",
              "      <td>Wouldn't even let a pregnant woman use the bat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>wouldnt even let a pregnant woman use the bath...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385397</th>\n",
              "      <td>Best pizza in the neighborhood!!! Love the thi...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>best pizza in the neighborhood love the this c...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>385398 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  stars  \\\n",
              "0       Was not that great. We sat for like 15 mins be...    2.0   \n",
              "1       My brother in law suggested this place, and we...    1.0   \n",
              "2       We were seated immediately but unfortunately i...    1.0   \n",
              "3       Here's a breakdown of what my boyfriend and I ...    2.0   \n",
              "4       Eating healthy I had the turkey burger it was ...    4.0   \n",
              "...                                                   ...    ...   \n",
              "385393  King burrito is my go to Burritos when I'm nea...    5.0   \n",
              "385394  Wow, such a lovely spot right off the highway ...    5.0   \n",
              "385395  Where do I start with Buranko? Everything I tr...    5.0   \n",
              "385396  Wouldn't even let a pregnant woman use the bat...    1.0   \n",
              "385397  Best pizza in the neighborhood!!! Love the thi...    5.0   \n",
              "\n",
              "                                          cleaned_reviews  sentiment  afinn  \n",
              "0       was not that great we sat for like 15 mins bef...       -1.0   13.0  \n",
              "1       my brother in law suggested this place and we ...       -1.0    0.0  \n",
              "2       we were seated immediately but unfortunately i...       -1.0    7.0  \n",
              "3       heres a breakdown of what my boyfriend and i s...       -1.0    0.0  \n",
              "4       eating healthy i had the turkey burger it was ...        1.0    7.0  \n",
              "...                                                   ...        ...    ...  \n",
              "385393  king burrito is my go to burritos when im near...        1.0   15.0  \n",
              "385394  wow such a lovely spot right off the highway t...        1.0   22.0  \n",
              "385395  where do i start with buranko everything i tri...        1.0    7.0  \n",
              "385396  wouldnt even let a pregnant woman use the bath...       -1.0   10.0  \n",
              "385397  best pizza in the neighborhood love the this c...        1.0    8.0  \n",
              "\n",
              "[385398 rows x 5 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "afinn = Afinn(language = 'en')\n",
        "review_df['afinn'] = review_df['text'].apply(afinn.score)\n",
        "review_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUW89FkqQvCG"
      },
      "outputs": [],
      "source": [
        "review_df['afinn_clean'] = review_df['cleaned_reviews'].apply(afinn.score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HG7TA3ZEWo8",
        "outputId": "5389b20d-0cf0-4dac-b1d2-89614adf8777"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>stars</th>\n",
              "      <th>cleaned_reviews</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>afinn</th>\n",
              "      <th>afinn_clean</th>\n",
              "      <th>afinn_pred</th>\n",
              "      <th>afinn_clean_pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Was not that great. We sat for like 15 mins be...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>was not that great we sat for like 15 mins bef...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>My brother in law suggested this place, and we...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>my brother in law suggested this place and we ...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>We were seated immediately but unfortunately i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>we were seated immediately but unfortunately i...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Here's a breakdown of what my boyfriend and I ...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>heres a breakdown of what my boyfriend and i s...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Eating healthy I had the turkey burger it was ...</td>\n",
              "      <td>4.0</td>\n",
              "      <td>eating healthy i had the turkey burger it was ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385393</th>\n",
              "      <td>King burrito is my go to Burritos when I'm nea...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>king burrito is my go to burritos when im near...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385394</th>\n",
              "      <td>Wow, such a lovely spot right off the highway ...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>wow such a lovely spot right off the highway t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385395</th>\n",
              "      <td>Where do I start with Buranko? Everything I tr...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>where do i start with buranko everything i tri...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385396</th>\n",
              "      <td>Wouldn't even let a pregnant woman use the bat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>wouldnt even let a pregnant woman use the bath...</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385397</th>\n",
              "      <td>Best pizza in the neighborhood!!! Love the thi...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>best pizza in the neighborhood love the this c...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>385398 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  stars  \\\n",
              "0       Was not that great. We sat for like 15 mins be...    2.0   \n",
              "1       My brother in law suggested this place, and we...    1.0   \n",
              "2       We were seated immediately but unfortunately i...    1.0   \n",
              "3       Here's a breakdown of what my boyfriend and I ...    2.0   \n",
              "4       Eating healthy I had the turkey burger it was ...    4.0   \n",
              "...                                                   ...    ...   \n",
              "385393  King burrito is my go to Burritos when I'm nea...    5.0   \n",
              "385394  Wow, such a lovely spot right off the highway ...    5.0   \n",
              "385395  Where do I start with Buranko? Everything I tr...    5.0   \n",
              "385396  Wouldn't even let a pregnant woman use the bat...    1.0   \n",
              "385397  Best pizza in the neighborhood!!! Love the thi...    5.0   \n",
              "\n",
              "                                          cleaned_reviews  sentiment  afinn  \\\n",
              "0       was not that great we sat for like 15 mins bef...       -1.0   13.0   \n",
              "1       my brother in law suggested this place and we ...       -1.0    0.0   \n",
              "2       we were seated immediately but unfortunately i...       -1.0    7.0   \n",
              "3       heres a breakdown of what my boyfriend and i s...       -1.0    0.0   \n",
              "4       eating healthy i had the turkey burger it was ...        1.0    7.0   \n",
              "...                                                   ...        ...    ...   \n",
              "385393  king burrito is my go to burritos when im near...        1.0   15.0   \n",
              "385394  wow such a lovely spot right off the highway t...        1.0   22.0   \n",
              "385395  where do i start with buranko everything i tri...        1.0    7.0   \n",
              "385396  wouldnt even let a pregnant woman use the bath...       -1.0   10.0   \n",
              "385397  best pizza in the neighborhood love the this c...        1.0    8.0   \n",
              "\n",
              "        afinn_clean  afinn_pred  afinn_clean_pred  \n",
              "0              13.0         1.0               1.0  \n",
              "1               0.0         0.0               0.0  \n",
              "2               7.0         1.0               1.0  \n",
              "3              -2.0         0.0              -1.0  \n",
              "4               7.0         1.0               1.0  \n",
              "...             ...         ...               ...  \n",
              "385393         15.0         1.0               1.0  \n",
              "385394         22.0         1.0               1.0  \n",
              "385395          7.0         1.0               1.0  \n",
              "385396          1.0         1.0               1.0  \n",
              "385397          8.0         1.0               1.0  \n",
              "\n",
              "[385398 rows x 8 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "review_df['afinn_pred'] = review_df['afinn'].apply(lambda x: np.sign(x))\n",
        "review_df['afinn_clean_pred'] = review_df['afinn_clean'].apply(lambda x: np.sign(x))\n",
        "review_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOe_f7Paaai-",
        "outputId": "fe539ff5-07ef-46b4-ac82-1d82d8785d15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "uncleaned accuracy: 0.8344853891301979\n",
            "cleaned accuracy: 0.836989294184194\n"
          ]
        }
      ],
      "source": [
        "print(f\"uncleaned accuracy: {len(review_df[review_df['sentiment']==review_df['afinn_pred']])/len(review_df)}\")\n",
        "print(f\"cleaned accuracy: {len(review_df[review_df['sentiment']==review_df['afinn_clean_pred']])/len(review_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCV_qehATalU"
      },
      "source": [
        "##Word-level RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH0CipJimzZd"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, test_df = \\\n",
        "              np.split(review_df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(review_df)), int(.8*len(review_df))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3ywBrvLjUMZ",
        "outputId": "3be2b32a-09ca-43ef-ddc9-1afa3c60ef39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ahfeng/anaconda3/envs/bigdata/bin/python\r\n"
          ]
        }
      ],
      "source": [
        "!which python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eIo327sk7Yj",
        "outputId": "02ae3f1d-0c2a-4807-d2af-8fbd749c0d89",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.11.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from torchtext==0.11.0) (4.62.3)\n",
            "Requirement already satisfied: requests in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from torchtext==0.11.0) (2.26.0)\n",
            "Requirement already satisfied: torch in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from torchtext==0.11.0) (1.10.0)\n",
            "Requirement already satisfied: numpy in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from torchtext==0.11.0) (1.21.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->torchtext==0.11.0) (3.3)\n",
            "Requirement already satisfied: typing_extensions in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from torch->torchtext==0.11.0) (3.10.0.2)\n",
            "Requirement already satisfied: sklearn in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /home/ahfeng/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.21.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.11.0\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHmZkSZbZxaG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "m4vFjG0ymzZe",
        "outputId": "06830c97-8496-4e6d-bc3a-27a15182f9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.2.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.9 MB 5.5 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.26.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
            "Requirement already satisfied: setuptools in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpk53I7PaOQq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp7otZNOTZxC"
      },
      "outputs": [],
      "source": [
        "# from https://github.com/bentrevett/pytorch-sentiment-analysis\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPg2ic3MmzZe"
      },
      "outputs": [],
      "source": [
        "# from https://gist.github.com/nissan/ccb0553edb6abafd20c3dec34ee8099d\n",
        "class DataFrameDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
        "        fields = [('text', text_field), ('label', label_field)]\n",
        "        examples = []\n",
        "        for i, row in df.iterrows():\n",
        "            label = row.sentiment if not is_test else None\n",
        "            text = row.text\n",
        "            examples.append(data.Example.fromlist([text, label], fields))\n",
        "\n",
        "        super().__init__(examples, fields, **kwargs)\n",
        "\n",
        "    @staticmethod\n",
        "    def sort_key(ex):\n",
        "        return len(ex.text)\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, text_field, label_field, train_df, val_df=None, test_df=None, **kwargs):\n",
        "        train_data, val_data, test_data = (None, None, None)\n",
        "\n",
        "        if train_df is not None:\n",
        "            train_data = cls(train_df.copy(), text_field, label_field, **kwargs)\n",
        "        if val_df is not None:\n",
        "            val_data = cls(val_df.copy(), text_field, label_field, **kwargs)\n",
        "        if test_df is not None:\n",
        "            test_data = cls(test_df.copy(), text_field, label_field, **kwargs) #keep is_text false since we have labels\n",
        "\n",
        "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-8XtxNthv8h"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "SEED = 42069\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cu28eCQmzZf"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data = DataFrameDataset.splits(text_field=TEXT, label_field=LABEL, train_df=train_df, val_df=val_df, test_df=test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok5vt62mzZg",
        "outputId": "0b9d9e15-f6d0-489f-a16b-21897e17cf6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ['As', 'far', 'as', 'Carl', \"'s\", 'Jr', 'goes', ',', 'I', \"'ve\", 'been', 'to', 'better', 'locations', '.', 'The', 'service', 'is', 'pretty', 'bad', 'here', 'and', 'there', \"'s\", 'about', 'a', '50', '%', 'chance', 'your', 'food', 'will', 'be', 'hot', 'and', 'fresh', '.', 'The', 'food', 'here', 'tastes', 'fine', ',', 'it', \"'s\", 'pretty', 'consistent', 'with', 'the', 'rest', 'of', 'the', 'Carl', \"'s\", 'Jr', 'in', 'the', 'area', '.'], 'label': -1.0}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHzrNbV6mzZg"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3wY17ajmzZg",
        "outputId": "d3025abf-9a50-4773-8699-775dc6bb5bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0eVP_zImzZg",
        "outputId": "b630fc1c-ed29-43cb-bd08-ee526e34ed4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "print(TEXT.vocab.stoi['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utvgqbJ5mzZg",
        "outputId": "851700f8-2d93-4770-815b-a1e67a517c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({1.0: 171804, -1.0: 59434})\n"
          ]
        }
      ],
      "source": [
        "print(LABEL.vocab.freqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wqa9fjFmzZh"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNxFfV9JmzZh"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTL_k5Q-mzZh",
        "outputId": "f72c6b28-2e1e-437e-e439-4d4810df56e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWaUVuYHmzZh"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-8x9dUdmzZh"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYhZqnV1mzZh"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6sp1XSamzZi"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LhoN4humzZi"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmHWeQOlmzZi"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUoO3bJOmzZi"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhfW9092mzZj",
        "outputId": "238e2a01-61a9-47fc-9ac0-2b57d41466e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 32s\n",
            "\tTrain Loss: 0.568 | Train Acc: 74.37%\n",
            "\t Val. Loss: 0.675 |  Val. Acc: 57.03%\n",
            "Epoch: 02 | Epoch Time: 1m 30s\n",
            "\tTrain Loss: 0.567 | Train Acc: 74.43%\n",
            "\t Val. Loss: 0.681 |  Val. Acc: 52.66%\n",
            "Epoch: 03 | Epoch Time: 1m 29s\n",
            "\tTrain Loss: 0.567 | Train Acc: 74.43%\n",
            "\t Val. Loss: 0.684 |  Val. Acc: 51.20%\n",
            "Epoch: 04 | Epoch Time: 1m 30s\n",
            "\tTrain Loss: 0.567 | Train Acc: 74.43%\n",
            "\t Val. Loss: 0.683 |  Val. Acc: 51.02%\n",
            "Epoch: 05 | Epoch Time: 1m 30s\n",
            "\tTrain Loss: 0.568 | Train Acc: 74.42%\n",
            "\t Val. Loss: 0.683 |  Val. Acc: 50.85%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXMW_CBFmzZj",
        "outputId": "a9b798b1-b0fe-44d0-c64a-4dc8037b14cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.676 | Test Acc: 56.72%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk_nyxjImzZj"
      },
      "source": [
        "##Improved RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQZuMUHdmzZj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        #pack sequence\n",
        "        # lengths need to be on CPU!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * num directions]\n",
        "        #output over padding tokens are zero tensors\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #cell = [num layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        #and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sofpxm3wmzZk"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy import data\n",
        "\n",
        "SEED = 69\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths = True)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqX6hY5wmzZk"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data = DataFrameDataset.splits(text_field=TEXT, label_field=LABEL, train_df=train_df, val_df=val_df, test_df=test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZNTIybgmzZk"
      },
      "outputs": [],
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv6GLZx4mzZk"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFU-zIvFmzZl"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlDO7ODXmzZl",
        "outputId": "b58bec11-c2fe-4550-ee37-aab4c0bca07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 4,810,857 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTNFwkJ7mzZl",
        "outputId": "03312566-b7d7-463b-f6a2-916328cfc74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ]
        }
      ],
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHKOkFrKmzZl",
        "outputId": "91674d6f-0163-43de-d208-4b80f44b2813"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.5300, -1.3035,  0.4438,  ...,  0.6058,  1.5191,  0.4225],\n",
              "        [-1.0078,  0.8051,  0.7826,  ..., -0.7693, -0.1897, -0.3624],\n",
              "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
              "        ...,\n",
              "        [ 0.8651,  1.3278, -0.7483,  ..., -2.5835,  0.2068,  0.8006],\n",
              "        [ 0.4914,  0.7875,  2.1520,  ...,  1.1039, -2.2060,  0.4389],\n",
              "        [ 0.4178,  1.0698, -1.3060,  ...,  1.0009,  0.9013,  0.2731]])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrCKPSbDmzZm",
        "outputId": "0c79f358-32b3-4e0f-b7ac-fb618931316c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],\n",
            "        ...,\n",
            "        [ 0.8651,  1.3278, -0.7483,  ..., -2.5835,  0.2068,  0.8006],\n",
            "        [ 0.4914,  0.7875,  2.1520,  ...,  1.1039, -2.2060,  0.4389],\n",
            "        [ 0.4178,  1.0698, -1.3060,  ...,  1.0009,  0.9013,  0.2731]])\n"
          ]
        }
      ],
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2t-S85imzZm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZVQ08PHmzZm"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoOYb8ksmzZm"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        text, text_lengths = batch.text\n",
        "        \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFqt3RpCmzZm"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            \n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qpoTSVrmzZo"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNNvjaWpmzZo",
        "outputId": "9cb2f1c0-e10d-419b-ca53-1d2bdc26f623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 24s\n",
            "\tTrain Loss: 0.222 | Train Acc: 90.72%\n",
            "\t Val. Loss: 0.112 |  Val. Acc: 95.91%\n",
            "Epoch: 02 | Epoch Time: 1m 25s\n",
            "\tTrain Loss: 0.107 | Train Acc: 96.00%\n",
            "\t Val. Loss: 0.089 |  Val. Acc: 96.86%\n",
            "Epoch: 03 | Epoch Time: 1m 26s\n",
            "\tTrain Loss: 0.085 | Train Acc: 96.84%\n",
            "\t Val. Loss: 0.079 |  Val. Acc: 97.07%\n",
            "Epoch: 04 | Epoch Time: 1m 24s\n",
            "\tTrain Loss: 0.074 | Train Acc: 97.30%\n",
            "\t Val. Loss: 0.083 |  Val. Acc: 97.03%\n",
            "Epoch: 05 | Epoch Time: 1m 24s\n",
            "\tTrain Loss: 0.066 | Train Acc: 97.62%\n",
            "\t Val. Loss: 0.074 |  Val. Acc: 97.33%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUNxLH7imzZo",
        "outputId": "6e312d0e-22d0-4156-c18c-2d03a7bf6bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.074 | Test Acc: 97.35%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po38AvmWmzZo"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfJp2KQOmzZo",
        "outputId": "6836f79e-3a7d-42b7-b94a-4387a67bf833"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9976223111152649"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sentiment(model, \"This film is terrible\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnqebT7DmzZp",
        "outputId": "bea8083a-5b5a-4d98-ff41-83a204c6d0ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6374624371528625"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_sentiment(model, \"What the fuck did you just fucking say about me, you little bitch? I'll have you know I graduated top of my class in the Navy Seals, and I've been involved in numerous secret raids on Al-Quaeda, and I have over 300 confirmed kills. \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bRYnPKfmzZp"
      },
      "source": [
        "##Bert Embedding RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wNMjLBwtmzZp",
        "outputId": "bc40c787-6d8e-4c11-9446-556401405ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (4.13.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: sacremoses in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: filelock in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: requests in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (1.21.4)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: joblib in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
            "Requirement already satisfied: six in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_6sSbyimzZp",
        "outputId": "f30ae945-1619-4475-c187-aaed098f9c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (7.6.5)\n",
            "Requirement already satisfied: widgetsnbextension in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (3.5.2)\n",
            "Requirement already satisfied: pandas-profiling in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (3.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (1.0.2)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (6.4.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipywidgets) (7.29.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from widgetsnbextension) (6.4.6)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas-profiling) (1.3.4)\n",
            "Requirement already satisfied: PyYAML>=5.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (6.0)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (3.0.2)\n",
            "Requirement already satisfied: visions[type_image_path]==0.7.4 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (0.7.4)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (1.21.4)\n",
            "Requirement already satisfied: joblib~=1.0.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (1.0.1)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas-profiling) (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas-profiling) (1.7.3)\n",
            "Requirement already satisfied: pydantic>=1.8.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (1.8.2)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (4.62.3)\n",
            "Requirement already satisfied: multimethod>=1.4 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (1.6)\n",
            "Requirement already satisfied: htmlmin>=0.1.12 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (0.1.12)\n",
            "Requirement already satisfied: markupsafe~=2.0.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (2.0.1)\n",
            "Requirement already satisfied: tangled-up-in-unicode==0.1.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (0.1.0)\n",
            "Requirement already satisfied: phik>=0.11.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (0.12.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas-profiling) (3.5.0)\n",
            "Requirement already satisfied: requests>=2.24.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pandas-profiling) (2.26.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.2.0)\n",
            "Requirement already satisfied: Pillow in /home/ahfeng/.local/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (8.4.0)\n",
            "Requirement already satisfied: imagehash in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (4.2.1)\n",
            "Requirement already satisfied: tornado<7.0,>=4.2 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.6)\n",
            "Requirement already satisfied: backcall in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: decorator in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: pickleshare in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: pygments in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
            "Requirement already satisfied: setuptools>=18.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from ipython>=4.0.0->ipywidgets) (58.0.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.1)\n",
            "Requirement already satisfied: entrypoints in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/ahfeng/.local/lib/python3.9/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ahfeng/.local/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (4.28.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ahfeng/.local/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ahfeng/.local/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
            "Requirement already satisfied: setuptools-scm>=4 in /home/ahfeng/.local/lib/python3.9/site-packages (from matplotlib>=3.2.0->pandas-profiling) (6.3.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: six>=1.11.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
            "Requirement already satisfied: argon2-cffi in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension) (20.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension) (0.9.4)\n",
            "Requirement already satisfied: nbconvert in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension) (6.1.0)\n",
            "Requirement already satisfied: prometheus-client in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from notebook>=4.4.1->widgetsnbextension) (0.12.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /home/ahfeng/.local/lib/python3.9/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2021.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests>=2.24.0->pandas-profiling) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests>=2.24.0->pandas-profiling) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests>=2.24.0->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from requests>=2.24.0->pandas-profiling) (2.0.4)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /home/ahfeng/.local/lib/python3.9/site-packages (from setuptools-scm>=4->matplotlib>=3.2.0->pandas-profiling) (1.2.2)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension) (2.21)\n",
            "Requirement already satisfied: PyWavelets in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.2.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (0.8.4)\n",
            "Requirement already satisfied: testpath in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (0.5.0)\n",
            "Requirement already satisfied: bleach in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (4.0.0)\n",
            "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (0.5.3)\n",
            "Requirement already satisfied: jupyterlab-pygments in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (0.1.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension) (0.7.1)\n",
            "Requirement already satisfied: async-generator in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension) (1.10)\n",
            "Requirement already satisfied: webencodings in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets widgetsnbextension pandas-profiling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwI5mqormzZp",
        "outputId": "229210aa-eef2-4df4-8014-34b280b5aceb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enabling notebook extension jupyter-js-widgets/extension...\r\n",
            "      - Validating: \u001b[32mOK\u001b[0m\r\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76vy1kB2mzZp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2DFgX8qmzZp",
        "outputId": "23722135-7350-4f56-bcad-12178e6d8775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: IProgress in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (0.4)\r\n",
            "Requirement already satisfied: six in /home/ahfeng/anaconda3/envs/bigdata/lib/python3.9/site-packages (from IProgress) (1.16.0)\r\n"
          ]
        }
      ],
      "source": [
        "!pip install IProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVqSKLHFmzZp"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "from ipywidgets import FloatProgress\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_SfFBngmzZ2",
        "outputId": "b1706461-627d-4e4b-d449-95165cdc5b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] [SEP] [PAD] [UNK]\n"
          ]
        }
      ],
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token\n",
        "\n",
        "print(init_token, eos_token, pad_token, unk_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKHvVO9QmzZ3",
        "outputId": "76b4340d-d366-4602-b019-161ab934abd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101 102 0 100\n"
          ]
        }
      ],
      "source": [
        "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
        "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
        "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
        "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
        "\n",
        "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbNCcbcLmzZ3",
        "outputId": "996481aa-8c86-4855-ddd8-d04d407cf64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n"
          ]
        }
      ],
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "\n",
        "print(max_input_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdmouzm-mzZ3"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhJnIcUpmzZ3"
      },
      "outputs": [],
      "source": [
        "from torchtext.legacy import data\n",
        "\n",
        "TEXT = data.Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjUMMmgFmzZ3"
      },
      "outputs": [],
      "source": [
        "train_data, val_data, test_data = DataFrameDataset.splits(text_field=TEXT, label_field=LABEL, train_df=train_df, val_df=val_df, test_df=test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLfSsKwlmzZ3",
        "outputId": "a27a8b64-bd6d-466e-ce8b-df4262809471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 231238\n",
            "Number of validation examples: 77080\n",
            "Number of testing examples: 77080\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(val_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy8sImtQmzZ3",
        "outputId": "d913149b-7a3b-499d-aaff-687fbd41a1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': [2307, 3962, 2000, 6865, 2041, 2007, 2814, 1998, 2131, 1037, 4658, 6700, 3334, 11529, 3347, 21209, 1012, 1996, 25545, 2003, 2428, 4658, 1011, 1996, 3681, 2031, 13297, 3645, 2007, 22243, 3221, 1012, 1996, 27612, 2024, 6625, 1012, 2256, 8241, 2001, 2200, 5379, 1012, 1045, 2347, 1005, 1056, 2205, 10326, 2006, 1996, 4777, 1006, 1996, 28774, 8717, 3211, 14894, 2001, 2205, 11259, 1007, 2021, 1996, 8974, 2020, 9805, 18879, 1012], 'label': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[6]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caXn5iBdmzZ3",
        "outputId": "02b98404-90f4-42c2-d7d5-9f5f8a05e338"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['great', 'spot', 'to', 'hang', 'out', 'with', 'friends', 'and', 'get', 'a', 'cool', 'hips', '##ter', 'dive', 'bar', 'vibe', '.', 'the', 'decor', 'is', 'really', 'cool', '-', 'the', 'walls', 'have', 'airplane', 'windows', 'with', 'mirrored', 'glass', '.', 'the', 'booths', 'are', 'comfortable', '.', 'our', 'server', 'was', 'very', 'friendly', '.', 'i', 'wasn', \"'\", 't', 'too', 'keen', 'on', 'the', 'wings', '(', 'the', 'ter', '##iya', '##ki', 'flavor', 'was', 'too', 'subtle', ')', 'but', 'the', 'drinks', 'were', 'yu', '##mmy', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['text'])\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa7OBBnGmzZ4"
      },
      "outputs": [],
      "source": [
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2Df3BTDmzZ4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, val_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data), \n",
        "    batch_size = BATCH_SIZE, \n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWdfPuwRmzZ4",
        "outputId": "905ec2f8-cfb3-4185-e14b-2f4ca91a8369"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpsMkM9omzZ4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERTGRUSentiment(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_dim,\n",
        "                 output_dim,\n",
        "                 n_layers,\n",
        "                 bidirectional,\n",
        "                 dropout):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert = bert\n",
        "        \n",
        "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "        \n",
        "        self.rnn = nn.GRU(embedding_dim,\n",
        "                          hidden_dim,\n",
        "                          num_layers = n_layers,\n",
        "                          bidirectional = bidirectional,\n",
        "                          batch_first = True,\n",
        "                          dropout = 0 if n_layers < 2 else dropout)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "        \n",
        "        #text = [batch size, sent len]\n",
        "                \n",
        "        with torch.no_grad():\n",
        "            embedded = self.bert(text)[0]\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        _, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [n layers * n directions, batch size, emb dim]\n",
        "        \n",
        "        if self.rnn.bidirectional:\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "        else:\n",
        "            hidden = self.dropout(hidden[-1,:,:])\n",
        "                \n",
        "        #hidden = [batch size, hid dim]\n",
        "        \n",
        "        output = self.out(hidden)\n",
        "        \n",
        "        #output = [batch size, out dim]\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XemQxrRmzZ4"
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.25\n",
        "\n",
        "model = BERTGRUSentiment(bert,\n",
        "                         HIDDEN_DIM,\n",
        "                         OUTPUT_DIM,\n",
        "                         N_LAYERS,\n",
        "                         BIDIRECTIONAL,\n",
        "                         DROPOUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-LzBnLXmzZ5",
        "outputId": "d7a475f5-4b2c-4112-faf1-45182410e19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 112,241,409 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSxayAbMmzZ5"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if name.startswith('bert'):\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nb0xIEomzZ5",
        "outputId": "b160f456-54f7-45b5-b389-21f231f51d21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 2,759,169 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObX9joUBmzZ6",
        "outputId": "b3ba42a0-7cbc-4505-912a-123bd1be32c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rnn.weight_ih_l0\n",
            "rnn.weight_hh_l0\n",
            "rnn.bias_ih_l0\n",
            "rnn.bias_hh_l0\n",
            "rnn.weight_ih_l0_reverse\n",
            "rnn.weight_hh_l0_reverse\n",
            "rnn.bias_ih_l0_reverse\n",
            "rnn.bias_hh_l0_reverse\n",
            "rnn.weight_ih_l1\n",
            "rnn.weight_hh_l1\n",
            "rnn.bias_ih_l1\n",
            "rnn.bias_hh_l1\n",
            "rnn.weight_ih_l1_reverse\n",
            "rnn.weight_hh_l1_reverse\n",
            "rnn.bias_ih_l1_reverse\n",
            "rnn.bias_hh_l1_reverse\n",
            "out.weight\n",
            "out.bias\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():                \n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj8ciwVRmzZ6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plyF6aY8mzZ7"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgbHepIDmzZ7"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPfRZ-wimzZ8"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeqnqxOCmzZ8"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8Om8HcumzZ8"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0TmXzPmzZ8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJavU82RmzZ8"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuL6BmSEmzZ8",
        "outputId": "ba35462a-c2b1-4b02-b977-01e8524ff09e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 47m 44s\n",
            "\tTrain Loss: 0.083 | Train Acc: 96.91%\n",
            "\t Val. Loss: 0.076 |  Val. Acc: 97.17%\n",
            "Epoch: 02 | Epoch Time: 47m 59s\n",
            "\tTrain Loss: 0.074 | Train Acc: 97.27%\n",
            "\t Val. Loss: 0.098 |  Val. Acc: 96.31%\n",
            "Epoch: 03 | Epoch Time: 48m 46s\n",
            "\tTrain Loss: 0.068 | Train Acc: 97.48%\n",
            "\t Val. Loss: 0.070 |  Val. Acc: 97.49%\n",
            "Epoch: 04 | Epoch Time: 48m 20s\n",
            "\tTrain Loss: 0.064 | Train Acc: 97.66%\n",
            "\t Val. Loss: 0.069 |  Val. Acc: 97.42%\n",
            "Epoch: 05 | Epoch Time: 47m 35s\n",
            "\tTrain Loss: 0.062 | Train Acc: 97.73%\n",
            "\t Val. Loss: 0.074 |  Val. Acc: 97.40%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    val_loss, val_acc = evaluate(model, val_iterator, criterion)\n",
        "        \n",
        "    end_time = time.time()\n",
        "        \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U5DIDSLmzZ9",
        "outputId": "cda95032-432d-444d-b76c-7b3ca97a52fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.069 | Test Acc: 97.42%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP5aQQxbmzZ9",
        "outputId": "73bef0b6-d513-422f-fbd5-1eca5495e15e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.069 | Test Acc: 97.42%\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut6-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BigDataAnalyticsProjectNotebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "bigdata",
      "language": "python",
      "name": "bigdata"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}